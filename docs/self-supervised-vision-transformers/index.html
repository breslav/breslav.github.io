<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0,user-scalable=0" />
<link rel="shortcut icon" href="https://breslav.github.io/images/favicon-32x32.png" />
<title>Self-Supervised Vision Transformers | Mikhail Breslav</title>
<meta name="title" content="Self-Supervised Vision Transformers" />
<meta name="description" content="Self-Supervised Vision Transformers (DINO):
One paper I&rsquo;ve been wanting to understand, at least at a high level, is DINO. DINO is another &ldquo;learn a good representation of images&rdquo; through self-supervision paper. In order to understand DINO I had to first take a detour into some contrastive learning literature which I wrote about here.
Let&rsquo;s jump right in! Below is a figure showing the broad architecture of DINO.









	
	
	
	
	 Main Architecture of DINO (from the paper). 
	
	
	
  " />
<meta name="keywords" content="" />


<meta property="og:url" content="https://breslav.github.io/self-supervised-vision-transformers/">
  <meta property="og:site_name" content="Mikhail Breslav">
  <meta property="og:title" content="Self-Supervised Vision Transformers">
  <meta property="og:description" content="Self-Supervised Vision Transformers (DINO): One paper I’ve been wanting to understand, at least at a high level, is DINO. DINO is another “learn a good representation of images” through self-supervision paper. In order to understand DINO I had to first take a detour into some contrastive learning literature which I wrote about here.
Let’s jump right in! Below is a figure showing the broad architecture of DINO. Main Architecture of DINO (from the paper).">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-02-21T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-21T00:00:00+00:00">
    <meta property="og:image" content="https://breslav.github.io/images/share.png">




  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://breslav.github.io/images/share.png">
  <meta name="twitter:title" content="Self-Supervised Vision Transformers">
  <meta name="twitter:description" content="Self-Supervised Vision Transformers (DINO): One paper I’ve been wanting to understand, at least at a high level, is DINO. DINO is another “learn a good representation of images” through self-supervision paper. In order to understand DINO I had to first take a detour into some contrastive learning literature which I wrote about here.
Let’s jump right in! Below is a figure showing the broad architecture of DINO. Main Architecture of DINO (from the paper).">




  <meta itemprop="name" content="Self-Supervised Vision Transformers">
  <meta itemprop="description" content="Self-Supervised Vision Transformers (DINO): One paper I’ve been wanting to understand, at least at a high level, is DINO. DINO is another “learn a good representation of images” through self-supervision paper. In order to understand DINO I had to first take a detour into some contrastive learning literature which I wrote about here.
Let’s jump right in! Below is a figure showing the broad architecture of DINO. Main Architecture of DINO (from the paper).">
  <meta itemprop="datePublished" content="2025-02-21T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-02-21T00:00:00+00:00">
  <meta itemprop="wordCount" content="631">
  <meta itemprop="image" content="https://breslav.github.io/images/share.png">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: white;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  figure {
    float: left;
    margin-top: 0.2cm;
    margin-left: auto;
    margin-right: 0.5cm;
  }


  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #eee;
  }

  pre code {
    border-left: 1px solid #999;
    color: #555;
    display: block;
    padding: 10px;
    white-space: pre-wrap;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #555;
    padding-left: 10px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

  h4 {
    margin-bottom: -0.25cm;
}
</style>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ND7MED44L2"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ND7MED44L2');
        }
      </script>

  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
  
</head>

<body>
  <header><a href="/" class="title">
  <h2>Mikhail Breslav</h2>
</a>
<nav><a href="/">Home</a>

<a href="/blog">Blog</a>
</nav>
</header>
  <main>

<h1>Self-Supervised Vision Transformers</h1>
<p>
  <i>
    <time datetime='2025-02-21' pubdate>
      21 Feb, 2025
    </time>
  </i>
</p>

<content>
  <h4 id="self-supervised-vision-transformers-dino">Self-Supervised Vision Transformers (DINO):</h4>
<p>One paper I&rsquo;ve been wanting to understand, at least at a high level, is <a href="https://arxiv.org/pdf/2104.14294" target="_blank" rel="noopener">DINO</a>. DINO is another &ldquo;learn a good representation of images&rdquo; through self-supervision paper. In order to understand DINO I had to first take a detour into some contrastive learning literature which I wrote about <a href="https://breslav.github.io/oops-contrastive-representation-learning/">here</a>.</p>
<p>Let&rsquo;s jump right in! Below is a figure showing the broad architecture of DINO.








<figure style="padding: 0.25rem; margin: 0.25rem 0; background-color: #ffffff; float: none; display: block;text-align: center;">
	<img style="width: auto; height: auto; float: none; display: block; margin-left: auto; margin-right: auto;text-align: center;" src="/self-supervised-vision-transformers/dino_architecture_hu11314465206177212890.png" width="400" height="364" alt="DINO Architecture">
	<figcaption>
	<small>
	
	 Main Architecture of DINO (from the paper). 
	
	</small>
	</figcaption>
</figure>  </p>
<p>If you are familiar with <a href="https://breslav.github.io/oops-contrastive-representation-learning/#byol">BYOL</a> you will notice that this architecture is very similar. What are the similarities?</p>
<ul>
<li>Once again we have a source image \(x\) that is used to generate two different views/augmentations \(x_{1}\) and \(x_{2}\) (in this work there is a set of views).</li>
<li>The views are fed into a student and teacher respectively which have the same architecture but different parameters.</li>
<li>The teacher parameters are based on an exponential moving average of the student parameters.</li>
<li>Only the student learns.</li>
</ul>
<p>Some areas where DINO is different than BYOL:</p>
<ul>
<li>DINO uses a vision transformer for the encoder.</li>
<li>The DINO the architecture is simpler in that the student (corresponding to the oneline network) and teacher (target network) don&rsquo;t connect to additional neural networks.</li>
<li>The teacher encoder outputs are fed through a centering operation which shifts the logits (there is also sharpening during the softmax).</li>
<li>The student and teacher both generate probability distributions over \(K\) &ldquo;classes&rdquo; which feed into a cross entropy loss that is used for training.</li>
<li>In DINO, a set of views are generated during augmentation and they can be more global (big crop) or local (small crop).</li>
<li>Multiple views are fed through the student and teacher with the caveat that the teacher only gets the global views.</li>
</ul>
<p>Related to the last bullet, an interesting aspect to the loss function used in this work is that it is a sum of cross entropy losses. That means the teacher is outputting probability distributions for all of the global views/augmentations and those are compared with probability distributions output by the student for all of the views. In general this is encouraging the student to match the teacher even if the student only sees a small part of the image. My interpretation is that the model is forced to learn the essence of the image by predicting that essence from smaller pieces. Kind of like predicting the theme of a jig-saw puzzle from smaller pieces.</p>
<p>Another interesting result from this paper is the visualization of attention maps from multiple heads. The way these maps are produced is by taking the encoder output corresponding to the [CLS] token and comparing it with the encoder outputs corresponding to different image patches.  In the figure below we see these attention maps with different colors representing different attention heads. What&rsquo;s impressive is how much the attention map looks like a segmentation map. In the first (top-left) image we see the model attends most strongly to the food on the plate and that different attention heads focus on different foods.</p>








<figure style="padding: 0.25rem; margin: 0.25rem 0; background-color: #ffffff; float: none; display: block;text-align: center;">
	<img style="width: auto; height: auto; float: none; display: block; margin-left: auto; margin-right: auto;text-align: center;" src="/self-supervised-vision-transformers/dino_attention_hu16346322753343210390.png" width="700" height="716" alt="DINO Attention Maps">
	<figcaption>
	<small>
	
	 Attention Map Visualizations (from the paper). 
	
	</small>
	</figcaption>
</figure>  
<h4 id="conclusion">Conclusion</h4>
<p>DINO demonstrates that you can learn a powerful representation of images by marrying the vision transformer with a contrastive-learning-like model without the need for labels. DINO also shows us through attention maps that transformers learn representations which reflect the key/salient objects in the image.</p>
<p>That said, I still find it surprising that these self-supervised models rooted in contrastive learning actually work. It kind of feels like all we are telling the model is that two different but related images should be encoded in a similar way and that somehow this is enough for the model to learn. The answer may just be with enough scale simple-ish models can do amazing things.</p>
<h4 id="references">References</h4>
<ul>
<li><a href="https://arxiv.org/pdf/2104.14294" target="_blank" rel="noopener">DINO</a></li>
<li><a href="https://www.youtube.com/watch?v=h3ij3F3cPIk" target="_blank" rel="noopener">DINO YT Video</a> Once again thanks to Yannic Kilcher for his great videos!</li>
</ul>

</content>
<p>
  
</p>

  </main>
  <footer><small>
  Copyright &copy; 2025 Mikhail Breslav 
</small></footer>

    
</body>

</html>
