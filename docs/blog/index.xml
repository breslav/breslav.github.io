<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Mikhail Breslav</title>
    <link>https://breslav.github.io/blog/</link>
    <description>Recent content in Blog on Mikhail Breslav</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <copyright>Copyright Â© 2025, Mikhail Breslav.....</copyright>
    <lastBuildDate>Fri, 07 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://breslav.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Brief Notes on Attention Efficiency</title>
      <link>https://breslav.github.io/brief-notes-on-attention-efficiency/</link>
      <pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://breslav.github.io/brief-notes-on-attention-efficiency/</guid>
      <description>&lt;p&gt;As part of my ongoing review of LLMs, I revisited the core computation performed during self attention. Like in my previous reviews, I focused on the&#xA;idea of there being three important learnable projections that map our token embeddings to queries, keys, and values which are then used to re-represent (add context to) the token embeddings. One aspect of attention that I glossed over in the past is the efficiency of this computation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Positional Embeddings are Strange</title>
      <link>https://breslav.github.io/positional-embeddings-are-strange/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://breslav.github.io/positional-embeddings-are-strange/</guid>
      <description>&lt;p&gt;Recently I&amp;rsquo;ve been reviewing the &amp;ldquo;basics&amp;rdquo; of large language models and decided to finally peek into the details of positional embeddings which I had ignored in the past. In this post I want to share what I&amp;rsquo;ve learned from reviewing this topic.&lt;/p&gt;&#xA;&lt;h4 id=&#34;positional-embedding-motivation&#34;&gt;Positional Embedding Motivation&lt;/h4&gt;&#xA;&lt;p&gt;In the foundational &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt; paper, positional embeddings are introduced as a way to add ordering information to token embeddings so that the transformer model has some way of understanding the order of the tokens. To state the somewhat obvious, we want language models to understand word order (and by extension token order) because word order impacts the semantics of what is being said.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Personal Website Reboot</title>
      <link>https://breslav.github.io/personal-website-reboot/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://breslav.github.io/personal-website-reboot/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s 2025 and I&amp;rsquo;ve decided it was time to reboot my personal website and blog which had both gone defunct over the years. My old &lt;a href=&#34;https://valserb.wordpress.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt; was started&#xA;when I began my PhD in 2010 and the last post to it was in 2018. Much of my old content centered around topics I was absorbing in grad school, like probabilistic graphical models, linear algebra, and general computing tools. A lot has happened since then and I hope to blog about some new topics that are on my mind soon, so stay tuned!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
